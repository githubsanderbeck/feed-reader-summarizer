'''
    ### Summary of the Program
    This Python script: 
        processes RSS feeds listed in an OPML file, 
        fetches recent articles within a user-specified time range, 
        scrapes their full content, generates 3-5 bullet-point summaries using the xAI Grok API, 
        outputs the results to both an HTML file and the console. 
        It uses libraries like `feedparser`, `requests`, `BeautifulSoup`, and `openai` for RSS parsing, web scraping, and API interaction.

    ---

    ### Key Functionality
    1. **Parse OPML File**: Extracts RSS feed URLs from an OPML file.
    2. **Fetch RSS Articles**: Retrieves articles from RSS feeds, filtering by publication date (within user-specified hours).
    3. **Scrape Article Content**: Fetches and extracts text from article URLs using web scraping.
    4. **Generate Summaries**: Uses the xAI Grok API to create 3-5 bullet-point summaries for each article.
    5. **Output Results**:
       - Saves articles with summaries to an HTML file in `~/jemp` with a timestamp (e.g., `articles_20250724_0952.html`).
       - Displays articles and summaries in the console for debugging.

    ---

    ### Configuration with `.env` File
    1. **Create a `.env` File**:
       - In the same directory as the script, create a `.env` file.
       - Add the xAI API key in the following format: XAI_API_KEY=your_xai_api_key_here
       - Obtain the API key from [xAI's API service](https://x.ai/api).
    2. **Purpose**:    - The script uses `load_dotenv()` to load the `XAI_API_KEY` for authenticating requests to the xAI Grok API.

    ### How to Run the Program
     python feedReader.py path/to/feeds.opml

    3. **User Input**:
       - The script prompts for the number of hours to go back for filtering articles (e.g., `24` for articles from the last 24 hours).
       - Enter a positive integer. Invalid inputs (e.g., negative numbers or non-integers) will prompt re-entry.

    4. **Execution Flow**:
       - Parses the OPML file to extract RSS feed URLs.
       - Fetches articles from each RSS feed, filtering by the specified time range.
       - Scrapes full article content from each URL.
       - Generates 3-5 bullet-point summaries using the Grok API.
       - Outputs results to an HTML file and the console.

    ### Output
    1. **HTML File**:
       - **Content**:
         - A styled HTML page listing articles sorted by publication date (newest first).
         - Each article includes:
           - Feed title (e.g., "BBC News").
           - Article title.
           - Article URL (clickable link).
           - Publication date (e.g., "2025-07-24 09:52:00").
           - 3-5 bullet-point summaries generated by Grok.
         - If no articles are found, displays: `No articles found within the specified time range.`
       - **Styling**: Clean, responsive design with borders, hover effects on links, and formatted summaries.

'''

import xml.etree.ElementTree as ET
import feedparser
from datetime import datetime, timedelta
import sys
import time
import os
from openai import OpenAI
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from typing import List
import json
import re
import requests
from bs4 import BeautifulSoup


# Parses an OPML file to extract a list of RSS feed URLs.
def get_rss_urls(opml_file):
    """Parse OPML file and return list of RSS feed URLs.
    
    Args:
        opml_file (str): Path to the OPML file containing RSS feed URLs.
    
    Returns:
        list: List of RSS feed URLs extracted from the OPML file.
    """
    try:
        tree = ET.parse(opml_file)
        root = tree.getroot()
        urls = []
        # Find all 'outline' elements with an 'xmlUrl' attribute (RSS feed URLs).
        for outline in root.findall(".//outline[@xmlUrl]"):
            url = outline.get('xmlUrl')
            if url:
                urls.append(url)
        return urls
    except Exception as e:
        print(f"Error parsing OPML file: {e}")
        return []

# Checks if an article's publication date is within the specified number of hours from the current time.
def is_within_hours(pub_date, current_date, hours):
    """Check if publication date is within the specified number of hours from the current time.
    
    Args:
        pub_date (datetime): Publication date of the article.
        current_date (datetime): Current date and time for comparison.
        hours (int): Number of hours to go back from the current time.
    
    Returns:
        bool: True if the publication date is within the specified time range, False otherwise.
    """
    time_threshold = current_date - timedelta(hours=hours)
    return pub_date >= time_threshold

# Fetches the full text content of an article by scraping its URL.
def fetch_article_content(url):
    """Fetch and extract text content from an article URL.
    
    Args:
        url (str): URL of the article to scrape.
    
    Returns:
        str: Extracted and cleaned article text, limited to 5000 characters, or an error message if scraping fails.
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Try to extract text from common content tags (e.g., <p>, <article>, <div> with specific classes).
        content = soup.find_all(['p', 'article', 'div'], class_=['content', 'article', 'post', 'entry-content'])
        if not content:
            content = soup.find_all('p')  # Fallback to all paragraphs if no specific content tags found.
        text = ' '.join(element.get_text(strip=True) for element in content)
        
        # Clean text by removing extra whitespace and limit to 5000 characters to stay within Grok's token limits.
        text = re.sub(r'\s+', ' ', text).strip()
        return text[:5000] if text else "No content extracted"
    except Exception as e:
        print(f"Error fetching article content from {url}: {e}")
        return "No content extracted"

# Defines a Pydantic model to validate Grok API responses, ensuring 3-5 bullet points.
class SummarySchema(BaseModel):
    bullet_points: List[str] = Field(..., min_items=3, max_items=5)

# Generates a 3-5 bullet-point summary using the Grok API, with fallbacks for errors.
def generate_bullet_points(content, title, client, model="grok-2"):
    """Generate bullet-point summary using Grok API with fallback.
    
    Args:
        content (str): Full article content or RSS summary.
        title (str): Article title for fallback if content is unavailable.
        client (OpenAI): Initialized OpenAI client for xAI API.
        model (str): Grok model to use (default: grok-2).
    
    Returns:
        list: List of 3-5 bullet-point strings summarizing the article.
    """
    if not content and not title:
        return ["No content available for summary"] * 3
    
    # Use article content if available, otherwise fallback to title.
    input_text = content if content != "No content extracted" else title
    # Instruct Grok to return a JSON object with 3-5 detailed bullet points.
    prompt = (
        f"Summarize the following article content in 3-5 bullet points as a JSON object with a 'bullet_points' key. "
        f"Each bullet point should be a detailed, concise string capturing key information, including main arguments, events, or findings. "
        f"Content: {input_text[:5000]}\n"
        f"Example output: {{'bullet_points': ['Point 1 with detail', 'Point 2 with detail', 'Point 3 with detail', 'Point 4 with detail']}}"
    )
    models = [model, "grok-4"]  # Try grok-2 first, fallback to grok-4 if it fails.
    for current_model in models:
        try:
            response = client.chat.completions.create(
                model=current_model,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant that generates detailed summaries in JSON format with a 'bullet_points' key."},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.3,  # Slightly higher temperature for detailed responses.
                max_tokens=400  # Allows longer, detailed summaries.
            )
            raw_response = response.choices[0].message.content
            parsed_response = json.loads(raw_response)
            
            # Fix response if Grok returns 'summary' instead of 'bullet_points'.
            if "bullet_points" not in parsed_response and "summary" in parsed_response:
                parsed_response = {"bullet_points": parsed_response["summary"][:5]}
            
            validated_response = SummarySchema(**parsed_response)
            bullet_points = validated_response.bullet_points
            # Ensure at least 3 bullet points by padding if necessary.
            while len(bullet_points) < 3:
                bullet_points.append("Additional detail not available")
            return bullet_points
        except Exception as e:
            print(f"Error generating summary with Grok (model: {current_model}): {e}")
            if current_model == models[-1]:  # Last model tried.
                # Fallback: split content into sentences or use title.
                if content and content != "No content extracted":
                    sentences = [s.strip() for s in re.split(r'[.!?]', content) if s.strip()]
                    return (sentences[:5] + ["Additional detail not available"] * (3 - len(sentences)))[:5]
                return ["No summary available", title[:200], "Additional detail not available"] * 2
    return ["No summary available due to API errors"] * 3

# Fetches articles from RSS feeds, filters by specified hours, and generates summaries.
def fetch_articles(urls, client, hours):
    """Fetch articles from RSS feeds and filter by specified hours from current time.
    
    Args:
        urls (list): List of RSS feed URLs.
        client (OpenAI): Initialized OpenAI client for xAI API.
        hours (int): Number of hours to go back for filtering articles.
    
    Returns:
        list: List of article dictionaries with title, link, publication date, feed title, and bullet-point summaries.
    """
    current_date = datetime.now()
    articles = []
    
    for url in urls:
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries:
                pub_date = None
                # Extract publication date from 'published_parsed' or 'updated_parsed'.
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    pub_date = datetime.fromtimestamp(time.mktime(entry.published_parsed))
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    pub_date = datetime.fromtimestamp(time.mktime(entry.updated_parsed))
                
                # Filter articles to those published within the specified hours.
                if pub_date and is_within_hours(pub_date, current_date, hours):
                    article_url = entry.get('link', '#')
                    article = {
                        'title': entry.get('title', 'No Title'),
                        'link': article_url,
                        'published': pub_date,
                        'feed_title': feed.feed.get('title', 'Unknown Feed'),
                        'summary': entry.get('summary', entry.get('description', ''))
                    }
                    # Fetch full article content and generate 3-5 bullet-point summaries.
                    article_content = fetch_article_content(article_url)
                    article['bullet_points'] = generate_bullet_points(article_content, article['title'], client)
                    articles.append(article)
        except Exception as e:
            print(f"Error processing feed {url}: {e}")
    
    return articles

# Generates an HTML file with article details and bullet-point summaries.
def generate_html(articles, output_file):
    """Generate HTML file with articles and bullet-point summaries.
    
    Args:
        articles (list): List of article dictionaries.
        output_file (str): Path to the output HTML file.
    """
    html_content = """
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Recent RSS Articles</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        h1 { color: #333; }
        .article { margin-bottom: 20px; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }
        .feed-title { color: #555; font-size: 1.1em; }
        .article-title { font-size: 1.4em; margin: 10px 0; }
        .article-link { color: #0066cc; text-decoration: none; }
        .article-link:hover { text-decoration: underline; }
        .published { color: #666; font-size: 0.9em; }
        .summary { margin-top: 10px; }
        .summary ul { margin: 5px 0; padding-left: 20px; }
    </style>
</head>
<body>
    <h1>Recent Articles</h1>
"""
    
    if not articles:
        html_content += "<p>No articles found within the specified time range.</p>"
    else:
        # Sort articles by publication date (newest first).
        articles.sort(key=lambda x: x['published'], reverse=True)
        for article in articles:
            html_content += f"""
    <div class="article">
        <div class="feed-title">{article['feed_title']}</div>
        <div class="article-title">{article['title']}</div>
        <div><a class="article-link" href="{article['link']}" target="_blank">{article['link']}</a></div>
        <div class="published">Published: {article['published'].strftime('%Y-%m-%d %H:%M:%S')}</div>
        <div class="summary">
            <strong>Summary:</strong>
            <ul>
                {"".join(f"<li>{point}</li>" for point in article['bullet_points'])}
            </ul>
        </div>
    </div>
"""
    
    html_content += """
</body>
</html>
"""
    
    # Write the HTML content to the output file.
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(html_content)

# Displays articles in the console for debugging purposes.
def display_articles(articles):
    """Display articles in console for debugging.
    
    Args:
        articles (list): List of article dictionaries.
    """
    if not articles:
        print("No articles found within the specified time range.")
        return
    
    # Sort articles by publication date (newest first).
    articles.sort(key=lambda x: x['published'], reverse=True)
    print("\nRecent articles:")
    print("-" * 50)
    for article in articles:
        print(f"Feed: {article['feed_title']}")
        print(f"Title: {article['title']}")
        print(f"Link: {article['link']}")
        print(f"Published: {article['published'].strftime('%Y-%m-%d %H:%M:%S')}")
        print("Summary:")
        for point in article['bullet_points']:
            print(f"  - {point}")
        print("-" * 50)

# Main function to orchestrate the script's execution.
def main():
    """Main function to process RSS feeds, generate summaries, and output results.
    
    Expects a single command-line argument: the path to an OPML file.
    Prompts the user for the number of hours to go back for filtering articles.
    Fetches RSS feeds, extracts articles within the specified time range,
    scrapes article content, generates summaries using Grok, and outputs to HTML and console.
    """
    # Check for correct command-line argument.
    if len(sys.argv) != 2:
        print("Usage: python read_opml.py <path_to_opml_file>")
        sys.exit(1)
    
    # Prompt user for number of hours to go back.
    while True:
        try:
            hours = int(input("Enter the number of hours to go back for article filtering (e.g., 24 for last day): "))
            if hours <= 0:
                print("Please enter a positive integer.")
                continue
            break
        except ValueError:
            print("Please enter a valid integer.")
    
    # Load API key from .env file.
    load_dotenv()
    xai_api_key = os.getenv("XAI_API_KEY")
    if not xai_api_key:
        print("Error: XAI_API_KEY not found in .env file")
        sys.exit(1)
    
    # Initialize OpenAI client for xAI API.
    client = OpenAI(
        api_key=xai_api_key,
        base_url="https://api.x.ai/v1"
    )
    
    # Parse OPML file to get RSS feed URLs.
    opml_file = sys.argv[1]
    rss_urls = get_rss_urls(opml_file)
    if not rss_urls:
        print("No RSS feeds found in the OPML file.")
        sys.exit(1)
    
    # Fetch articles and generate summaries for the specified time range.
    articles = fetch_articles(rss_urls, client, hours)
    
    # Generate HTML output with datestamp including minutes and seconds (e.g., articles_20250724_0952.html)
    current_date = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir = os.path.expanduser("~/jemp")
    os.makedirs(output_dir, exist_ok=True)  # Create ~/jemp directory if it doesn't exist
    output_file = os.path.join(output_dir, f"articles_{current_date}.html")
    generate_html(articles, output_file)
    print(f"HTML output generated: {output_file}")
    
    # Display articles in console for debugging.
    display_articles(articles)

if __name__ == "__main__":
    main()
